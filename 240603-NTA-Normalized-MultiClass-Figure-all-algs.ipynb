{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97fcdc54",
   "metadata": {},
   "source": [
    "### Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2f042",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654483c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries to open data file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pprint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7debefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in NTA Normalized dataset(92 samples x 593 features). This is post converting log10 normalization. All 0s are 1/10th lowest conc of the entire dataset (Stults cite)\n",
    "dat_nt = pd.read_csv(r'240603-NTA-Normalized-MultiClass-Input.csv', header=0)\n",
    "#print(dat_nt.head)\n",
    "#print(dat_nt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0000456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to numpy\n",
    "dat_nt_np = dat_nt.to_numpy()\n",
    "target_1 = dat_nt_np[:,0] #Convert target variables to 2D-array for sci-kit learn\n",
    "data_1 = dat_nt_np[:,1:]\n",
    "#print(target_1.shape)\n",
    "#print(data_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86defcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710, 720, 730, 740, 750, 760, 770, 780, 790, 800, 810, 820, 830, 840, 850, 860, 870, 880, 890, 900, 910, 920, 930, 940, 950, 960, 970, 980, 990, 1000, 1010, 1020, 1030, 1040, 1050, 1060, 1070, 1080, 1090, 1100, 1110, 1120, 1130, 1140, 1150, 1160, 1170, 1180, 1190, 1200, 1210, 1220, 1230, 1240, 1250, 1260, 1270, 1280, 1290, 1300, 1310, 1320, 1330, 1340, 1350, 1360, 1370, 1380, 1390, 1400, 1410, 1420, 1430, 1440, 1450, 1460, 1470, 1480, 1490, 1500, 1510, 1520, 1530, 1540, 1550, 1560, 1570, 1580, 1590, 1600, 1610, 1620, 1630, 1640, 1650, 1660, 1670, 1680, 1690, 1700, 1710, 1720, 1730, 1740, 1750, 1760, 1770, 1780, 1790, 1800, 1810, 1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "{'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710, 720, 730, 740, 750, 760, 770, 780, 790, 800, 810, 820, 830, 840, 850, 860, 870, 880, 890, 900, 910, 920, 930, 940, 950, 960, 970, 980, 990, 1000, 1010, 1020, 1030, 1040, 1050, 1060, 1070, 1080, 1090, 1100, 1110, 1120, 1130, 1140, 1150, 1160, 1170, 1180, 1190, 1200, 1210, 1220, 1230, 1240, 1250, 1260, 1270, 1280, 1290, 1300, 1310, 1320, 1330, 1340, 1350, 1360, 1370, 1380, 1390, 1400, 1410, 1420, 1430, 1440, 1450, 1460, 1470, 1480, 1490, 1500, 1510, 1520, 1530, 1540, 1550, 1560, 1570, 1580, 1590, 1600, 1610, 1620, 1630, 1640, 1650, 1660, 1670, 1680, 1690, 1700, 1710, 1720, 1730, 1740, 1750, 1760, 1770, 1780, 1790, 1800, 1810, 1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "#Running randomized CV to narrow grid for GridSearch CV\n",
    "#Modified from here: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 2000, num = 200)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4218fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 100 candidates, totalling 600 fits\n",
      "Fitting 6 folds for each of 100 candidates, totalling 600 fits\n",
      "Best Parameters: {'n_estimators': 1070, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None, 'bootstrap': False}\n",
      "Best Parameters: {'n_estimators': 1070, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and held-out validation set\n",
    "data_train, data_val, target_train, target_val = train_test_split(data_1, target_1, test_size=0.2, random_state=42, stratify=target_1)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model_rnd = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=model_rnd, param_distributions=random_grid, n_iter=100, cv=6, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model on the training data\n",
    "rf_random.fit(data_train, target_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b78b57e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Balanced Accuracy: 0.8194444444444443\n",
      "Validation Balanced Accuracy: 0.8194444444444443\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the held-out validation set\n",
    "best_model = rf_random.best_estimator_\n",
    "val_predictions = best_model.predict(data_val)\n",
    "val_balanced_accuracy = balanced_accuracy_score(target_val, val_predictions)\n",
    "\n",
    "print(\"Validation Balanced Accuracy:\", val_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and held-out validation set\n",
    "data_train, data_val, target_train, target_val = train_test_split(data_1, target_1, test_size=0.2, random_state=42, stratify=target_1)\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Parameters to explore\n",
    "n_estimators = [1060, 1070]\n",
    "max_features = ['log2']\n",
    "max_depth = [100, None]\n",
    "min_samples_leaf = [1, 2]\n",
    "min_samples_split = [5, 6]\n",
    "bootstrap = [False]\n",
    "\n",
    "# Create a custom scorer for balanced accuracy\n",
    "balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Define the grid\n",
    "grid = dict(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, bootstrap=bootstrap)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=6, n_repeats=10, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=balanced_accuracy_scorer, error_score=0)\n",
    "grid_result = grid_search.fit(data_train, target_train.ravel())\n",
    "\n",
    "# Summarize results of training data\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Evaluate the best model on the held-out validation set\n",
    "best_model = grid_result.best_estimator_\n",
    "val_predictions = best_model.predict(data_val)\n",
    "val_balanced_accuracy = balanced_accuracy_score(target_val, val_predictions)\n",
    "\n",
    "print(\"Validation Balanced Accuracy:\", val_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now running optimized model over the entire dataset\n",
    "model = RandomForestClassifier(n_estimators = 1060, max_features = 'log2', max_depth = 100, min_samples_leaf = 1, min_samples_split = 5, bootstrap = False)\n",
    "# fit the model\n",
    "model.fit(data_1, target_1.ravel())\n",
    "acc = model.score(data_1, target_1)\n",
    "\n",
    "print(\"RF accuracy: \" + str(round(acc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels for the data\n",
    "predicted_labels = model.predict(data_1)\n",
    "\n",
    "# Initialize lists to store data points, row numbers, and original target labels for each class\n",
    "class_data = [[] for _ in range(6)]  # Assuming 6 classes\n",
    "class_row_numbers = [[] for _ in range(6)]\n",
    "class_original_labels = [[] for _ in range(6)]\n",
    "\n",
    "# Organize samples into their respective classes\n",
    "for row_number, (predicted_label, original_label) in enumerate(zip(predicted_labels, target_1)):\n",
    "    class_data[int(predicted_label)].append(data_1[row_number])\n",
    "    class_row_numbers[int(predicted_label)].append(row_number)\n",
    "    class_original_labels[int(predicted_label)].append(original_label)\n",
    "\n",
    "# Print information for each class\n",
    "for class_idx in range(6):\n",
    "    print(f\"Class {class_idx}:\")\n",
    "    print(f\"Number of data points: {len(class_data[class_idx])}\")\n",
    "    print(f\"Row numbers: {class_row_numbers[class_idx]}\")\n",
    "    print(f\"Original target labels: {class_original_labels[class_idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from class numbers to class types\n",
    "class_mapping = {\n",
    "    0.0: \"AFFF-GW\",\n",
    "    1.0: \"LL\",\n",
    "    2.0: \"BL\",\n",
    "    3.0: \"WWTP\",\n",
    "    4.0: \"PP\",\n",
    "    5.0: \"PG\"\n",
    "}\n",
    "\n",
    "# Replace class numbers with class types in class_original_labels\n",
    "class_original_labels = [\n",
    "    [class_mapping.get(label, label) for label in class_labels]\n",
    "    for class_labels in class_original_labels\n",
    "]\n",
    "print(class_original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38aafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class types in the same order as the mapping\n",
    "class_types = [\"AFFF-GW\", \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"]\n",
    "\n",
    "# Initialize six NumPy arrays to count instances for each class type\n",
    "y1 = np.zeros(len(class_original_labels))\n",
    "y2 = np.zeros(len(class_original_labels))\n",
    "y3 = np.zeros(len(class_original_labels))\n",
    "y4 = np.zeros(len(class_original_labels))\n",
    "y5 = np.zeros(len(class_original_labels))\n",
    "y6 = np.zeros(len(class_original_labels))\n",
    "\n",
    "# Iterate through class_original_labels to count instances for each class type\n",
    "for i, labels in enumerate(class_original_labels):\n",
    "    for j, class_type in enumerate(class_types):\n",
    "        # Count the instances of the current class type in the current class\n",
    "        count = labels.count(class_type)\n",
    "        \n",
    "        # Update the respective NumPy array (e.g., y1, y2, etc.)\n",
    "        if j == 0:\n",
    "            y1[i] = count\n",
    "        elif j == 1:\n",
    "            y2[i] = count\n",
    "        elif j == 2:\n",
    "            y3[i] = count\n",
    "        elif j == 3:\n",
    "            y4[i] = count\n",
    "        elif j == 4:\n",
    "            y5[i] = count\n",
    "        elif j == 5:\n",
    "            y6[i] = count\n",
    "            \n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y4)\n",
    "print(y5)\n",
    "print(y6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b519728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"RF Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "#plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('RF-MultiClass-RNKU-Normalized_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"RF Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('RF-MultiClass-RNKU-Normalized-Legend_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c913fce",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running randomized CV to narrow grid for GridSearch CV\n",
    "#Modified from here: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l1','l2']\n",
    "C = np.logspace(-2, 10, 13)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'solver': solver,\n",
    "               'penalty': penalty,\n",
    "               'C': C.tolist()}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246867cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and held-out validation set\n",
    "data_train, data_val, target_train, target_val = train_test_split(data_1, target_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model_rnd = LogisticRegression()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "lr_random = RandomizedSearchCV(estimator=model_rnd, param_distributions=random_grid, n_iter=100, cv=7, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model on the training data\n",
    "lr_random.fit(data_train, target_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", lr_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the held-out validation set\n",
    "best_model = lr_random.best_estimator_\n",
    "val_predictions = best_model.predict(data_val)\n",
    "val_balanced_accuracy = balanced_accuracy_score(target_val, val_predictions)\n",
    "\n",
    "print(\"Validation Balanced Accuracy:\", val_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bc252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and held-out validation set\n",
    "data_train, data_val, target_train, target_val = train_test_split(data_1, target_1, test_size=0.2, random_state=42)\n",
    "\n",
    "#Defining model based on results from randomized CV\n",
    "model = LogisticRegression()\n",
    "\n",
    "#Parameters to explore\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "C = [0.1, 10, 100]\n",
    "\n",
    "#Create a custom scorer for balanced accuracy\n",
    "balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "#Defining grid and performing CV of training data\n",
    "grid = dict(solver=solver, penalty=penalty, C=C)\n",
    "cv = RepeatedStratifiedKFold(n_splits=7, n_repeats=10, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=balanced_accuracy_scorer, error_score=0)\n",
    "grid_result = grid_search.fit(data_train, target_train.ravel())\n",
    "\n",
    "# Summarize results of training data\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Evaluate the best model on the held-out validation set\n",
    "best_model = grid_result.best_estimator_\n",
    "val_predictions = best_model.predict(data_val)\n",
    "val_balanced_accuracy = balanced_accuracy_score(target_val, val_predictions)\n",
    "\n",
    "print(\"Validation Balanced Accuracy:\", val_balanced_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8635a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now running optimized model over the entire dataset\n",
    "model = LogisticRegression(C=10, penalty = 'l2', solver='newton-cg')\n",
    "# fit the model\n",
    "model.fit(data_1, target_1.ravel())\n",
    "acc = model.score(data_1, target_1)\n",
    "\n",
    "print(\"LR accuracy: \" + str(round(acc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004caa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels for the data\n",
    "predicted_labels = model.predict(data_1)\n",
    "\n",
    "# Initialize lists to store data points, row numbers, and original target labels for each class\n",
    "class_data = [[] for _ in range(6)]  # Assuming 6 classes\n",
    "class_row_numbers = [[] for _ in range(6)]\n",
    "class_original_labels = [[] for _ in range(6)]\n",
    "\n",
    "# Organize samples into their respective classes\n",
    "for row_number, (predicted_label, original_label) in enumerate(zip(predicted_labels, target_1)):\n",
    "    class_data[int(predicted_label)].append(data_1[row_number])\n",
    "    class_row_numbers[int(predicted_label)].append(row_number)\n",
    "    class_original_labels[int(predicted_label)].append(original_label)\n",
    "\n",
    "# Print information for each class\n",
    "for class_idx in range(6):\n",
    "    print(f\"Class {class_idx}:\")\n",
    "    print(f\"Number of data points: {len(class_data[class_idx])}\")\n",
    "    print(f\"Row numbers: {class_row_numbers[class_idx]}\")\n",
    "    print(f\"Original target labels: {class_original_labels[class_idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1843258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from class numbers to class types\n",
    "class_mapping = {\n",
    "    0.0: \"AFFF-GW\",\n",
    "    1.0: \"LL\",\n",
    "    2.0: \"BL\",\n",
    "    3.0: \"WWTP\",\n",
    "    4.0: \"PP\",\n",
    "    5.0: \"PG\"\n",
    "}\n",
    "\n",
    "# Replace class numbers with class types in class_original_labels\n",
    "class_original_labels = [\n",
    "    [class_mapping.get(label, label) for label in class_labels]\n",
    "    for class_labels in class_original_labels\n",
    "]\n",
    "print(class_original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class types in the same order as the mapping\n",
    "class_types = [\"AFFF-GW\", \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"]\n",
    "\n",
    "# Initialize six NumPy arrays to count instances for each class type\n",
    "y1 = np.zeros(len(class_original_labels))\n",
    "y2 = np.zeros(len(class_original_labels))\n",
    "y3 = np.zeros(len(class_original_labels))\n",
    "y4 = np.zeros(len(class_original_labels))\n",
    "y5 = np.zeros(len(class_original_labels))\n",
    "y6 = np.zeros(len(class_original_labels))\n",
    "\n",
    "# Iterate through class_original_labels to count instances for each class type\n",
    "for i, labels in enumerate(class_original_labels):\n",
    "    for j, class_type in enumerate(class_types):\n",
    "        # Count the instances of the current class type in the current class\n",
    "        count = labels.count(class_type)\n",
    "        \n",
    "        # Update the respective NumPy array (e.g., y1, y2, etc.)\n",
    "        if j == 0:\n",
    "            y1[i] = count\n",
    "        elif j == 1:\n",
    "            y2[i] = count\n",
    "        elif j == 2:\n",
    "            y3[i] = count\n",
    "        elif j == 3:\n",
    "            y4[i] = count\n",
    "        elif j == 4:\n",
    "            y5[i] = count\n",
    "        elif j == 5:\n",
    "            y6[i] = count\n",
    "            \n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y4)\n",
    "print(y5)\n",
    "print(y6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04616693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"LR Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "#plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('LR-MultiClass-RNKU-Normalized_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"LR Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('LR-MultiClass-RNKU-Normalized-Legend_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d40882",
   "metadata": {},
   "source": [
    "### Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc237f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running randomized CV to narrow grid for GridSearch CV\n",
    "#Modified from here: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "C_range = np.logspace(-5, 10, 16)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'C': C_range.tolist()}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and held-out validation set\n",
    "data_train, data_val, target_train, target_val = train_test_split(data_1, target_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model_rnd = LogisticRegression()\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model_rnd = SVC(kernel = 'linear', tol = 0.00001, shrinking = True, cache_size = 200, verbose = False, max_iter = -1)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "sv_random = RandomizedSearchCV(estimator = model_rnd, param_distributions = random_grid, n_iter = 16, cv = 7, verbose=2, random_state=42, n_jobs = -1, error_score = 0)\n",
    "# Fit the random search model on the training data\n",
    "sv_random.fit(data_train, target_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", sv_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9be6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the held-out validation set\n",
    "sv_model = sv_random.best_estimator_\n",
    "val_predictions = sv_model.predict(data_val)\n",
    "val_balanced_accuracy = balanced_accuracy_score(target_val, val_predictions)\n",
    "\n",
    "print(\"Validation Balanced Accuracy:\", val_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now running optimized model over the entire dataset\n",
    "model = SVC(kernel = 'linear', C = 0.01, tol = 0.00001, shrinking = True, cache_size = 200, verbose = False, max_iter = -1)\n",
    "# fit the model\n",
    "model.fit(data_1, target_1.ravel())\n",
    "acc = model.score(data_1, target_1)\n",
    "\n",
    "print(\"SVC accuracy: \" + str(round(acc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels for the data\n",
    "predicted_labels = model.predict(data_1)\n",
    "\n",
    "# Initialize lists to store data points, row numbers, and original target labels for each class\n",
    "class_data = [[] for _ in range(6)]  # Assuming 6 classes\n",
    "class_row_numbers = [[] for _ in range(6)]\n",
    "class_original_labels = [[] for _ in range(6)]\n",
    "\n",
    "# Organize samples into their respective classes\n",
    "for row_number, (predicted_label, original_label) in enumerate(zip(predicted_labels, target_1)):\n",
    "    class_data[int(predicted_label)].append(data_1[row_number])\n",
    "    class_row_numbers[int(predicted_label)].append(row_number)\n",
    "    class_original_labels[int(predicted_label)].append(original_label)\n",
    "\n",
    "# Print information for each class\n",
    "for class_idx in range(6):\n",
    "    print(f\"Class {class_idx}:\")\n",
    "    print(f\"Number of data points: {len(class_data[class_idx])}\")\n",
    "    print(f\"Row numbers: {class_row_numbers[class_idx]}\")\n",
    "    print(f\"Original target labels: {class_original_labels[class_idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from class numbers to class types\n",
    "class_mapping = {\n",
    "    0.0: \"AFFF-GW\",\n",
    "    1.0: \"LL\",\n",
    "    2.0: \"BL\",\n",
    "    3.0: \"WWTP\",\n",
    "    4.0: \"PP\",\n",
    "    5.0: \"PG\"\n",
    "}\n",
    "\n",
    "# Replace class numbers with class types in class_original_labels\n",
    "class_original_labels = [\n",
    "    [class_mapping.get(label, label) for label in class_labels]\n",
    "    for class_labels in class_original_labels\n",
    "]\n",
    "print(class_original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class types in the same order as the mapping\n",
    "class_types = [\"AFFF-GW\", \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"]\n",
    "\n",
    "# Initialize six NumPy arrays to count instances for each class type\n",
    "y1 = np.zeros(len(class_original_labels))\n",
    "y2 = np.zeros(len(class_original_labels))\n",
    "y3 = np.zeros(len(class_original_labels))\n",
    "y4 = np.zeros(len(class_original_labels))\n",
    "y5 = np.zeros(len(class_original_labels))\n",
    "y6 = np.zeros(len(class_original_labels))\n",
    "\n",
    "# Iterate through class_original_labels to count instances for each class type\n",
    "for i, labels in enumerate(class_original_labels):\n",
    "    for j, class_type in enumerate(class_types):\n",
    "        # Count the instances of the current class type in the current class\n",
    "        count = labels.count(class_type)\n",
    "        \n",
    "        # Update the respective NumPy array (e.g., y1, y2, etc.)\n",
    "        if j == 0:\n",
    "            y1[i] = count\n",
    "        elif j == 1:\n",
    "            y2[i] = count\n",
    "        elif j == 2:\n",
    "            y3[i] = count\n",
    "        elif j == 3:\n",
    "            y4[i] = count\n",
    "        elif j == 4:\n",
    "            y5[i] = count\n",
    "        elif j == 5:\n",
    "            y6[i] = count\n",
    "            \n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y4)\n",
    "print(y5)\n",
    "print(y6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"SVC Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "#plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('SVC-MultiClass-RNKU-Normalized_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"SVC Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('SVC-MultiClass-RNKU-Normalized-Legend_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7551bdd",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running randomized CV to narrow grid for GridSearch CV\n",
    "#Modified from here: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# Citerion to measure quality of a plit\n",
    "crit = ['gini', 'entropy']\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'criterion': crit,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and held-out validation set\n",
    "data_train, data_val, target_train, target_val = train_test_split(data_1, target_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model_rnd = DecisionTreeClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "dt_random = RandomizedSearchCV(estimator = model_rnd, param_distributions = random_grid, n_iter = 100, cv = 7, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "dt_random.fit(data_train, target_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", dt_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the held-out validation set\n",
    "dt_model = dt_random.best_estimator_\n",
    "val_predictions = dt_model.predict(data_val)\n",
    "val_balanced_accuracy = balanced_accuracy_score(target_val, val_predictions)\n",
    "\n",
    "print(\"Validation Balanced Accuracy:\", val_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and held-out validation set\n",
    "data_train, data_val, target_train, target_val = train_test_split(data_1, target_1, test_size=0.2, random_state=42)\n",
    "\n",
    "#Defining model based on results from randomized CV\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "#Parameters to explore\n",
    "min_samples_split = [1, 2]\n",
    "min_samples_leaf = [1, 2]\n",
    "max_features = ['sqrt']\n",
    "max_depth = [70, 80, 90]\n",
    "criterion = ['entropy']\n",
    "\n",
    "#Create a custom scorer for balanced accuracy\n",
    "balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "#Defining grid and performing CV of training data\n",
    "grid = dict(min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, max_features = max_features, max_depth = max_depth, criterion=criterion)\n",
    "cv = RepeatedStratifiedKFold(n_splits=7, n_repeats=100, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=balanced_accuracy_scorer, error_score=0)\n",
    "grid_result = grid_search.fit(data_train, target_train.ravel())\n",
    "\n",
    "# Summarize results of training data\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Evaluate the best model on the held-out validation set\n",
    "best_model = grid_result.best_estimator_\n",
    "val_predictions = best_model.predict(data_val)\n",
    "val_balanced_accuracy = balanced_accuracy_score(target_val, val_predictions)\n",
    "\n",
    "print(\"Validation Balanced Accuracy:\", val_balanced_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f899de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now running optimized model over the entire dataset\n",
    "model = DecisionTreeClassifier(min_samples_split = 2, min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, criterion='entropy')\n",
    "# fit the model\n",
    "model.fit(data_1, target_1.ravel())\n",
    "acc = model.score(data_1, target_1)\n",
    "\n",
    "print(\"DT accuracy: \" + str(round(acc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973aa3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels for the data\n",
    "predicted_labels = model.predict(data_1)\n",
    "\n",
    "# Initialize lists to store data points, row numbers, and original target labels for each class\n",
    "class_data = [[] for _ in range(6)]  # Assuming 6 classes\n",
    "class_row_numbers = [[] for _ in range(6)]\n",
    "class_original_labels = [[] for _ in range(6)]\n",
    "\n",
    "# Organize samples into their respective classes\n",
    "for row_number, (predicted_label, original_label) in enumerate(zip(predicted_labels, target_1)):\n",
    "    class_data[int(predicted_label)].append(data_1[row_number])\n",
    "    class_row_numbers[int(predicted_label)].append(row_number)\n",
    "    class_original_labels[int(predicted_label)].append(original_label)\n",
    "\n",
    "# Print information for each class\n",
    "for class_idx in range(6):\n",
    "    print(f\"Class {class_idx}:\")\n",
    "    print(f\"Number of data points: {len(class_data[class_idx])}\")\n",
    "    print(f\"Row numbers: {class_row_numbers[class_idx]}\")\n",
    "    print(f\"Original target labels: {class_original_labels[class_idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99337f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from class numbers to class types\n",
    "class_mapping = {\n",
    "    0.0: \"AFFF-GW\",\n",
    "    1.0: \"LL\",\n",
    "    2.0: \"BL\",\n",
    "    3.0: \"WWTP\",\n",
    "    4.0: \"PP\",\n",
    "    5.0: \"PG\"\n",
    "}\n",
    "\n",
    "# Replace class numbers with class types in class_original_labels\n",
    "class_original_labels = [\n",
    "    [class_mapping.get(label, label) for label in class_labels]\n",
    "    for class_labels in class_original_labels\n",
    "]\n",
    "print(class_original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class types in the same order as the mapping\n",
    "class_types = [\"AFFF-GW\", \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"]\n",
    "\n",
    "# Initialize six NumPy arrays to count instances for each class type\n",
    "y1 = np.zeros(len(class_original_labels))\n",
    "y2 = np.zeros(len(class_original_labels))\n",
    "y3 = np.zeros(len(class_original_labels))\n",
    "y4 = np.zeros(len(class_original_labels))\n",
    "y5 = np.zeros(len(class_original_labels))\n",
    "y6 = np.zeros(len(class_original_labels))\n",
    "\n",
    "# Iterate through class_original_labels to count instances for each class type\n",
    "for i, labels in enumerate(class_original_labels):\n",
    "    for j, class_type in enumerate(class_types):\n",
    "        # Count the instances of the current class type in the current class\n",
    "        count = labels.count(class_type)\n",
    "        \n",
    "        # Update the respective NumPy array (e.g., y1, y2, etc.)\n",
    "        if j == 0:\n",
    "            y1[i] = count\n",
    "        elif j == 1:\n",
    "            y2[i] = count\n",
    "        elif j == 2:\n",
    "            y3[i] = count\n",
    "        elif j == 3:\n",
    "            y4[i] = count\n",
    "        elif j == 4:\n",
    "            y5[i] = count\n",
    "        elif j == 5:\n",
    "            y6[i] = count\n",
    "            \n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y4)\n",
    "print(y5)\n",
    "print(y6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"DT Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "#plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('DT-MultiClass-RNKU-Normalized_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bab923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"DT Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('DT-MultiClass-RNKU-Normalized-Legend_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca7826",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02367e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running randomized CV to narrow grid for GridSearch CV\n",
    "#Modified from here: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "random_grid = {'gamma': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4, 200],\n",
    "              'learning_rate': [0.01, 0.03, 0.06, 0.1, 0.15, 0.2, 0.25, 0.300000012, 0.4, 0.5, 0.6, 0.7],\n",
    "              'max_depth': [5,6,7,8,9,10,11,12,13,14],\n",
    "              'n_estimators': [50,65,80,100,115,130,150],\n",
    "              'reg_alpha': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4,200],\n",
    "              'reg_lambda': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4,200]}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54950f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model_rnd = xgb.XGBClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = model_rnd, param_distributions = random_grid, n_iter = 100, cv = 7, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(data_1, target_1)\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78611194",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining model based on results from randomized CV\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "#Parameters to explore\n",
    "\n",
    "reg_lambda = [0.1, 0.2, 0.3]\n",
    "reg_alpha = [0.6, 0.8, 1.0]\n",
    "n_estimators = [50, 100]\n",
    "max_depth = [10, 11, 12]\n",
    "learning_rate = [0.1]\n",
    "gamma = [0, 0.2, 0.4]\n",
    "\n",
    "#Create a custom scorer for balanced accuracy\n",
    "balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "#Defining grid and performing CV of training data\n",
    "grid = dict(reg_lambda=reg_lambda, reg_alpha=reg_alpha, n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, gamma=gamma)\n",
    "cv = RepeatedStratifiedKFold(n_splits=7, n_repeats=100, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=balanced_accuracy_scorer, error_score=0)\n",
    "grid_result = grid_search.fit(data_1, target_1.ravel())\n",
    "\n",
    "# summarize results of training data\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now running optimized model over the entire dataset\n",
    "model = xgb.XGBClassifier(gamma = 0, learning_rate = 0.55, max_depth = 4, n_estimators = 60, reg_alpha = 0.6, reg_lambda = 13.2)\n",
    "# fit the model\n",
    "model.fit(data_1, target_1.ravel())\n",
    "acc = model.score(data_1, target_1)\n",
    "\n",
    "print(\"XGBC accuracy: \" + str(round(acc, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels for the data\n",
    "predicted_labels = model.predict(data_1)\n",
    "\n",
    "# Initialize lists to store data points, row numbers, and original target labels for each class\n",
    "class_data = [[] for _ in range(6)]  # Assuming 6 classes\n",
    "class_row_numbers = [[] for _ in range(6)]\n",
    "class_original_labels = [[] for _ in range(6)]\n",
    "\n",
    "# Organize samples into their respective classes\n",
    "for row_number, (predicted_label, original_label) in enumerate(zip(predicted_labels, target_1)):\n",
    "    class_data[int(predicted_label)].append(data_1[row_number])\n",
    "    class_row_numbers[int(predicted_label)].append(row_number)\n",
    "    class_original_labels[int(predicted_label)].append(original_label)\n",
    "\n",
    "# Print information for each class\n",
    "for class_idx in range(6):\n",
    "    print(f\"Class {class_idx}:\")\n",
    "    print(f\"Number of data points: {len(class_data[class_idx])}\")\n",
    "    print(f\"Row numbers: {class_row_numbers[class_idx]}\")\n",
    "    print(f\"Original target labels: {class_original_labels[class_idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c278f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from class numbers to class types\n",
    "class_mapping = {\n",
    "    0.0: \"AFFF-GW\",\n",
    "    1.0: \"LL\",\n",
    "    2.0: \"BL\",\n",
    "    3.0: \"WWTP\",\n",
    "    4.0: \"PP\",\n",
    "    5.0: \"PG\"\n",
    "}\n",
    "\n",
    "# Replace class numbers with class types in class_original_labels\n",
    "class_original_labels = [\n",
    "    [class_mapping.get(label, label) for label in class_labels]\n",
    "    for class_labels in class_original_labels\n",
    "]\n",
    "print(class_original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class types in the same order as the mapping\n",
    "class_types = [\"AFFF-GW\", \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"]\n",
    "\n",
    "# Initialize six NumPy arrays to count instances for each class type\n",
    "y1 = np.zeros(len(class_original_labels))\n",
    "y2 = np.zeros(len(class_original_labels))\n",
    "y3 = np.zeros(len(class_original_labels))\n",
    "y4 = np.zeros(len(class_original_labels))\n",
    "y5 = np.zeros(len(class_original_labels))\n",
    "y6 = np.zeros(len(class_original_labels))\n",
    "\n",
    "# Iterate through class_original_labels to count instances for each class type\n",
    "for i, labels in enumerate(class_original_labels):\n",
    "    for j, class_type in enumerate(class_types):\n",
    "        # Count the instances of the current class type in the current class\n",
    "        count = labels.count(class_type)\n",
    "        \n",
    "        # Update the respective NumPy array (e.g., y1, y2, etc.)\n",
    "        if j == 0:\n",
    "            y1[i] = count\n",
    "        elif j == 1:\n",
    "            y2[i] = count\n",
    "        elif j == 2:\n",
    "            y3[i] = count\n",
    "        elif j == 3:\n",
    "            y4[i] = count\n",
    "        elif j == 4:\n",
    "            y5[i] = count\n",
    "        elif j == 5:\n",
    "            y6[i] = count\n",
    "            \n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "print(y4)\n",
    "print(y5)\n",
    "print(y6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"XGBoost Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "#plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('XGBoost-MultiClass-RNKU-Normalized_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294de8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar plot\n",
    "x = ['AFFF-GW', 'LL', 'BL', 'WWTP', 'PP', 'PG']\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.bar(x, y1, color ='#7ad151')\n",
    "plt.bar(x, y2, bottom=y1, color='#22a884')\n",
    "plt.bar(x, y3, bottom=y1+y2, color='#fde725')\n",
    "plt.bar(x, y4, bottom=y1+y2+y3, color='#440154')\n",
    "plt.bar(x, y5, bottom=y1+y2+y3+y4, color='#414487')\n",
    "plt.bar(x, y6, bottom=y1+y2+y3+y4+y5, color='#2a788e')\n",
    "plt.yticks(np.arange(0, 29, 2), fontsize = 12)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.xlabel(\"XGBoost Groups\", fontsize = 14)\n",
    "plt.ylabel(\"Frequency\", fontsize = 14)\n",
    "for i, (yi, yj, yk, yl, ym, yn) in enumerate(zip(y1, y2, y3, y4, y5, y6)):\n",
    "    total_count = int(yi + yj + yk + yl + ym + yn)  # Convert to int to remove decimals\n",
    "    plt.text(i, total_count + 0.5, str(total_count), ha=\"center\")\n",
    "plt.legend([\"AFFF-GW\",  \"LL\", \"BL\", \"WWTP\", \"PP\", \"PG\"], loc=\"upper right\")\n",
    "#plt.title(\"Class breakdown via RF vs. Original Class Label\", fontsize = 16)\n",
    "plt.savefig('XGBoost-MultiClass-RNKU-Normalized-Legend_NEW.png', dpi = 1500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899e7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
